{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport sklearn.decomposition as skde\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nimport sklearn.metrics as sklm\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,roc_curve,roc_auc_score,classification_report\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"['sample_submission.csv', 'test.csv', 'train.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Import Datasets****\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import required datasets\ntest_data = pd.read_csv(\"../input/test.csv\")\ntrain_data= pd.read_csv(\"../input/train.csv\")\ntrain_data.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   ID_code  target    var_0   var_1   ...     var_196  var_197  var_198  var_199\n0  train_0       0   8.9255 -6.7863   ...      7.8784   8.5635  12.7803  -1.0914\n1  train_1       0  11.5006 -4.1473   ...      8.1267   8.7889  18.3560   1.9518\n2  train_2       0   8.6093 -2.7457   ...     -6.5213   8.2675  14.7222   0.3965\n3  train_3       0  11.0604 -2.1518   ...     -2.9275  10.2922  17.9697  -8.9996\n4  train_4       0   9.8369 -1.4834   ...      3.9267   9.5031  17.9974  -8.8104\n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>-4.9200</td>\n      <td>5.7470</td>\n      <td>2.9252</td>\n      <td>3.1821</td>\n      <td>14.0137</td>\n      <td>0.5745</td>\n      <td>8.7989</td>\n      <td>14.5691</td>\n      <td>5.7487</td>\n      <td>-7.2393</td>\n      <td>4.2840</td>\n      <td>30.7133</td>\n      <td>10.5350</td>\n      <td>16.2191</td>\n      <td>2.5791</td>\n      <td>2.4716</td>\n      <td>14.3831</td>\n      <td>13.4325</td>\n      <td>-5.1488</td>\n      <td>-0.4073</td>\n      <td>4.9306</td>\n      <td>5.9965</td>\n      <td>-0.3085</td>\n      <td>12.9041</td>\n      <td>-3.8766</td>\n      <td>16.8911</td>\n      <td>11.1920</td>\n      <td>10.5785</td>\n      <td>0.6764</td>\n      <td>7.8871</td>\n      <td>...</td>\n      <td>15.4576</td>\n      <td>5.3133</td>\n      <td>3.6159</td>\n      <td>5.0384</td>\n      <td>6.6760</td>\n      <td>12.6644</td>\n      <td>2.7004</td>\n      <td>-0.6975</td>\n      <td>9.5981</td>\n      <td>5.4879</td>\n      <td>-4.7645</td>\n      <td>-8.4254</td>\n      <td>20.8773</td>\n      <td>3.1531</td>\n      <td>18.5618</td>\n      <td>7.7423</td>\n      <td>-10.1245</td>\n      <td>13.7241</td>\n      <td>-3.5189</td>\n      <td>1.7202</td>\n      <td>-8.4051</td>\n      <td>9.0164</td>\n      <td>3.0657</td>\n      <td>14.3691</td>\n      <td>25.8398</td>\n      <td>5.8764</td>\n      <td>11.8411</td>\n      <td>-19.7159</td>\n      <td>17.5743</td>\n      <td>0.5857</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>3.1468</td>\n      <td>8.0851</td>\n      <td>-0.4032</td>\n      <td>8.0585</td>\n      <td>14.0239</td>\n      <td>8.4135</td>\n      <td>5.4345</td>\n      <td>13.7003</td>\n      <td>13.8275</td>\n      <td>-15.5849</td>\n      <td>7.8000</td>\n      <td>28.5708</td>\n      <td>3.4287</td>\n      <td>2.7407</td>\n      <td>8.5524</td>\n      <td>3.3716</td>\n      <td>6.9779</td>\n      <td>13.8910</td>\n      <td>-11.7684</td>\n      <td>-2.5586</td>\n      <td>5.0464</td>\n      <td>0.5481</td>\n      <td>-9.2987</td>\n      <td>7.8755</td>\n      <td>1.2859</td>\n      <td>19.3710</td>\n      <td>11.3702</td>\n      <td>0.7399</td>\n      <td>2.7995</td>\n      <td>5.8434</td>\n      <td>...</td>\n      <td>29.4846</td>\n      <td>5.8683</td>\n      <td>3.8208</td>\n      <td>15.8348</td>\n      <td>-5.0121</td>\n      <td>15.1345</td>\n      <td>3.2003</td>\n      <td>9.3192</td>\n      <td>3.8821</td>\n      <td>5.7999</td>\n      <td>5.5378</td>\n      <td>5.0988</td>\n      <td>22.0330</td>\n      <td>5.5134</td>\n      <td>30.2645</td>\n      <td>10.4968</td>\n      <td>-7.2352</td>\n      <td>16.5721</td>\n      <td>-7.3477</td>\n      <td>11.0752</td>\n      <td>-5.5937</td>\n      <td>9.4878</td>\n      <td>-14.9100</td>\n      <td>9.4245</td>\n      <td>22.5441</td>\n      <td>-4.8622</td>\n      <td>7.6543</td>\n      <td>-15.9319</td>\n      <td>13.3175</td>\n      <td>-0.3566</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>-4.9193</td>\n      <td>5.9525</td>\n      <td>-0.3249</td>\n      <td>-11.2648</td>\n      <td>14.1929</td>\n      <td>7.3124</td>\n      <td>7.5244</td>\n      <td>14.6472</td>\n      <td>7.6782</td>\n      <td>-1.7395</td>\n      <td>4.7011</td>\n      <td>20.4775</td>\n      <td>17.7559</td>\n      <td>18.1377</td>\n      <td>1.2145</td>\n      <td>3.5137</td>\n      <td>5.6777</td>\n      <td>13.2177</td>\n      <td>-7.9940</td>\n      <td>-2.9029</td>\n      <td>5.8463</td>\n      <td>6.1439</td>\n      <td>-11.1025</td>\n      <td>12.4858</td>\n      <td>-2.2871</td>\n      <td>19.0422</td>\n      <td>11.0449</td>\n      <td>4.1087</td>\n      <td>4.6974</td>\n      <td>6.9346</td>\n      <td>...</td>\n      <td>13.2070</td>\n      <td>5.8442</td>\n      <td>4.7086</td>\n      <td>5.7141</td>\n      <td>-1.0410</td>\n      <td>20.5092</td>\n      <td>3.2790</td>\n      <td>-5.5952</td>\n      <td>7.3176</td>\n      <td>5.7690</td>\n      <td>-7.0927</td>\n      <td>-3.9116</td>\n      <td>7.2569</td>\n      <td>-5.8234</td>\n      <td>25.6820</td>\n      <td>10.9202</td>\n      <td>-0.3104</td>\n      <td>8.8438</td>\n      <td>-9.7009</td>\n      <td>2.4013</td>\n      <td>-4.2935</td>\n      <td>9.3908</td>\n      <td>-13.2648</td>\n      <td>3.1545</td>\n      <td>23.0866</td>\n      <td>-5.3000</td>\n      <td>5.3745</td>\n      <td>-6.2660</td>\n      <td>10.1934</td>\n      <td>-0.8417</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>-5.8609</td>\n      <td>8.2450</td>\n      <td>2.3061</td>\n      <td>2.8102</td>\n      <td>13.8463</td>\n      <td>11.9704</td>\n      <td>6.4569</td>\n      <td>14.8372</td>\n      <td>10.7430</td>\n      <td>-0.4299</td>\n      <td>15.9426</td>\n      <td>13.7257</td>\n      <td>20.3010</td>\n      <td>12.5579</td>\n      <td>6.8202</td>\n      <td>2.7229</td>\n      <td>12.1354</td>\n      <td>13.7367</td>\n      <td>0.8135</td>\n      <td>-0.9059</td>\n      <td>5.9070</td>\n      <td>2.8407</td>\n      <td>-15.2398</td>\n      <td>10.4407</td>\n      <td>-2.5731</td>\n      <td>6.1796</td>\n      <td>10.6093</td>\n      <td>-5.9158</td>\n      <td>8.1723</td>\n      <td>2.8521</td>\n      <td>...</td>\n      <td>31.8833</td>\n      <td>5.9684</td>\n      <td>7.2084</td>\n      <td>3.8899</td>\n      <td>-11.0882</td>\n      <td>17.2502</td>\n      <td>2.5881</td>\n      <td>-2.7018</td>\n      <td>0.5641</td>\n      <td>5.3430</td>\n      <td>-7.1541</td>\n      <td>-6.1920</td>\n      <td>18.2366</td>\n      <td>11.7134</td>\n      <td>14.7483</td>\n      <td>8.1013</td>\n      <td>11.8771</td>\n      <td>13.9552</td>\n      <td>-10.4701</td>\n      <td>5.6961</td>\n      <td>-3.7546</td>\n      <td>8.4117</td>\n      <td>1.8986</td>\n      <td>7.2601</td>\n      <td>-0.4639</td>\n      <td>-0.0498</td>\n      <td>7.9336</td>\n      <td>-12.8279</td>\n      <td>12.4124</td>\n      <td>1.8489</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>6.2654</td>\n      <td>7.6784</td>\n      <td>-9.4458</td>\n      <td>-12.1419</td>\n      <td>13.8481</td>\n      <td>7.8895</td>\n      <td>7.7894</td>\n      <td>15.0553</td>\n      <td>8.4871</td>\n      <td>-3.0680</td>\n      <td>6.5263</td>\n      <td>11.3152</td>\n      <td>21.4246</td>\n      <td>18.9608</td>\n      <td>10.1102</td>\n      <td>2.7142</td>\n      <td>14.2080</td>\n      <td>13.5433</td>\n      <td>3.1736</td>\n      <td>-3.3423</td>\n      <td>5.9015</td>\n      <td>7.9352</td>\n      <td>-3.1582</td>\n      <td>9.4668</td>\n      <td>-0.0083</td>\n      <td>19.3239</td>\n      <td>12.4057</td>\n      <td>0.6329</td>\n      <td>2.7922</td>\n      <td>5.8184</td>\n      <td>...</td>\n      <td>33.5107</td>\n      <td>5.6953</td>\n      <td>5.4663</td>\n      <td>18.2201</td>\n      <td>6.5769</td>\n      <td>21.2607</td>\n      <td>3.2304</td>\n      <td>-1.7759</td>\n      <td>3.1283</td>\n      <td>5.5518</td>\n      <td>1.4493</td>\n      <td>-2.6627</td>\n      <td>19.8056</td>\n      <td>2.3705</td>\n      <td>18.4685</td>\n      <td>16.3309</td>\n      <td>-3.3456</td>\n      <td>13.5261</td>\n      <td>1.7189</td>\n      <td>5.1743</td>\n      <td>-7.6938</td>\n      <td>9.7685</td>\n      <td>4.8910</td>\n      <td>12.2198</td>\n      <td>11.8503</td>\n      <td>-7.8931</td>\n      <td>6.4209</td>\n      <td>5.9270</td>\n      <td>16.0201</td>\n      <td>-0.2829</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Explore Datasets**\n- View Data information\n- Check for missing values\n- Check correlation \n- Visualize class distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#view Data shape\nprint(train_data.shape,test_data.shape)\n#check for missing values\nprint(train_data.isnull().values.any(),test_data.isnull().values.any())\n#check data correlation \nprint(train_data.corr())\n#Visualize class distribution\nsns.countplot(train_data['target'])","execution_count":3,"outputs":[{"output_type":"stream","text":"(200000, 202) (200000, 201)\nFalse False\n           target     var_0     var_1    ...      var_197   var_198   var_199\ntarget   1.000000  0.052390  0.050343    ...    -0.035303 -0.053000  0.025434\nvar_0    0.052390  1.000000 -0.000544    ...    -0.000753 -0.005776  0.003850\nvar_1    0.050343 -0.000544  1.000000    ...    -0.004157 -0.004861  0.002287\nvar_2    0.055870  0.006573  0.003980    ...     0.001078 -0.000877  0.003855\nvar_3    0.011055  0.003801  0.000010    ...     0.001164 -0.001651  0.000506\nvar_4    0.010915  0.001326  0.000303    ...    -0.000046 -0.001821 -0.000786\nvar_5    0.030979  0.003046 -0.000902    ...    -0.000535 -0.000953  0.002767\nvar_6    0.066731  0.006983  0.003258    ...    -0.003565 -0.003025  0.006096\nvar_7   -0.003025  0.002429  0.001511    ...     0.003466  0.000650 -0.001457\nvar_8    0.019584  0.004962  0.004098    ...    -0.004583  0.002950  0.000854\nvar_9   -0.042805 -0.002613 -0.000832    ...     0.003701  0.002343  0.001070\nvar_10  -0.002213  0.000355  0.002875    ...     0.002680 -0.001546 -0.003561\nvar_11   0.022993  0.003468  0.004778    ...    -0.005642 -0.000968  0.000037\nvar_12  -0.069489 -0.001996 -0.001977    ...     0.002946  0.007545 -0.001016\nvar_13  -0.055156 -0.002717 -0.001303    ...    -0.004304 -0.001896 -0.003060\nvar_14  -0.006332 -0.004584 -0.001544    ...     0.003843 -0.000571 -0.004266\nvar_15   0.017283 -0.000043  0.004946    ...    -0.001167  0.000382  0.002613\nvar_16   0.008117  0.001112 -0.002481    ...     0.001704  0.002018 -0.000637\nvar_17   0.000864 -0.001664 -0.001180    ...    -0.000425  0.001190 -0.002404\nvar_18   0.043479  0.004292  0.000289    ...    -0.003269 -0.004619 -0.001419\nvar_19   0.011291  0.001505  0.001246    ...    -0.000325  0.001284 -0.002175\nvar_20  -0.018329 -0.000932 -0.002255    ...     0.000335  0.005043 -0.002770\nvar_21  -0.058483  0.000781 -0.001519    ...     0.002419  0.001164 -0.002598\nvar_22   0.060558  0.002322  0.001762    ...     0.002285 -0.005303  0.003568\nvar_23  -0.025473 -0.000987  0.003606    ...    -0.001586  0.001267  0.001140\nvar_24   0.028477  0.003658  0.001417    ...     0.004293  0.001555 -0.000527\nvar_25   0.013328  0.001158  0.000844    ...     0.002041 -0.001635  0.001498\nvar_26   0.062422  0.006117  0.004778    ...    -0.004430 -0.005124  0.003651\nvar_27  -0.000582  0.000985 -0.004551    ...     0.000005 -0.002254 -0.000011\nvar_28  -0.023942  0.001081 -0.002463    ...    -0.001648 -0.002587  0.001034\n...           ...       ...       ...    ...          ...       ...       ...\nvar_170  0.047973  0.002574  0.001036    ...     0.005171  0.000171  0.000970\nvar_171  0.014873  0.001678  0.004399    ...    -0.002478 -0.000382 -0.001210\nvar_172 -0.037976 -0.005587 -0.002802    ...     0.000754 -0.001549 -0.002997\nvar_173  0.042022  0.002499  0.002784    ...    -0.005428 -0.000410 -0.000286\nvar_174 -0.061669 -0.000683 -0.004758    ...    -0.000391  0.002679 -0.002023\nvar_175  0.021692 -0.000546  0.001693    ...    -0.003062  0.001439 -0.000117\nvar_176  0.007469  0.001197 -0.001331    ...    -0.000055  0.002079  0.001157\nvar_177 -0.036863  0.000953  0.002266    ...     0.000470  0.000910 -0.001324\nvar_178 -0.019681  0.001165 -0.002005    ...    -0.002700  0.003372 -0.003479\nvar_179  0.050002  0.003447  0.000883    ...     0.003045 -0.000959 -0.004120\nvar_180  0.031190  0.001939  0.003376    ...    -0.002536 -0.001624  0.004855\nvar_181  0.013714  0.001952  0.000008    ...     0.000755 -0.001818 -0.000330\nvar_182 -0.007198 -0.001957  0.001516    ...     0.001797 -0.001379  0.001640\nvar_183 -0.005467 -0.001721  0.001055    ...     0.000617  0.002719 -0.001917\nvar_184  0.048315 -0.000554 -0.000248    ...     0.001931 -0.004368  0.001839\nvar_185  0.000053  0.001868  0.002600    ...     0.000108 -0.000830 -0.000142\nvar_186 -0.030421  0.001267  0.002227    ...     0.003362  0.000047 -0.002140\nvar_187  0.014873  0.001413 -0.000236    ...     0.001030 -0.001298 -0.000715\nvar_188 -0.034015  0.001770 -0.002185    ...    -0.000296  0.001503 -0.002079\nvar_189  0.009212  0.000543  0.005840    ...     0.001096  0.000360  0.003081\nvar_190  0.055973  0.002752  0.006627    ...    -0.004974 -0.000153 -0.000404\nvar_191  0.047114  0.000206  0.003621    ...     0.000906 -0.000067  0.003595\nvar_192 -0.042858 -0.005373 -0.002604    ...    -0.000527  0.003451 -0.001239\nvar_193 -0.017709  0.001616  0.001153    ...     0.005068  0.001646 -0.000552\nvar_194 -0.022838 -0.001514 -0.002557    ...     0.000884  0.003194 -0.005615\nvar_195  0.028285  0.002073 -0.000785    ...    -0.004170 -0.000536  0.002042\nvar_196  0.023608  0.004386 -0.000377    ...    -0.000454  0.000253  0.000607\nvar_197 -0.035303 -0.000753 -0.004157    ...     1.000000  0.001183  0.004991\nvar_198 -0.053000 -0.005776 -0.004861    ...     0.001183  1.000000 -0.004731\nvar_199  0.025434  0.003850  0.002287    ...     0.004991 -0.004731  1.000000\n\n[201 rows x 201 columns]\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7fa487b55320>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Deductions from the above**\n- There are no missing vallues in either the test or train sets\n- The train set contains an extra column which is the known target to be used in training our model\n- There is no correlation between the features in the train dataset\n- There is a significant class imbalance between the 0s and 1s\n\nThe percentage of each class is calculated in the cell below"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check class distribution in percentage\ncount_0 = len(train_data[train_data[\"target\"] == 0])\ncount_1 = len(train_data[train_data[\"target\"] == 1])\npercentage_count_0 = ((count_0)/(count_0+count_1)) * 100\npercentage_count_1 = 100-percentage_count_0\nprint(\"{}{}{}{}{}\".format(\"Percentage of 0 class is \",percentage_count_0,\"\\n\",\"Percentage of 1 class is \",percentage_count_1))\n","execution_count":4,"outputs":[{"output_type":"stream","text":"Percentage of 0 class is 89.95100000000001\nPercentage of 1 class is 10.048999999999992\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Split train_data into train and validation sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train_data[\"target\"]\nnew_train_data = train_data.drop([\"target\",\"ID_code\"],axis =1)\nnew_train_data.head()\nx_train, x_test, y_train, y_test = train_test_split(new_train_data, labels, test_size = 0.25, random_state = 0)\nprint(x_train.shape,x_test.shape)\nprint(y_train.shape,y_test.shape)\nx_train.head()","execution_count":5,"outputs":[{"output_type":"stream","text":"(150000, 200) (50000, 200)\n(150000,) (50000,)\n","name":"stdout"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"          var_0   var_1    var_2   ...     var_197  var_198  var_199\n105800   7.1710  0.0780   9.7313   ...      8.4862  16.8159  -7.3853\n27538   12.9988 -0.8167   9.0578   ...      8.4979  13.6564  -1.1474\n108979  11.2861 -0.2255  17.2522   ...      7.3530  13.5594   6.4416\n184818  13.4434 -5.7222  13.4512   ...      8.5145  18.6113   3.9470\n121763   7.3488  3.2396  12.8417   ...      8.0920  16.3242 -10.1387\n\n[5 rows x 200 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>var_8</th>\n      <th>var_9</th>\n      <th>var_10</th>\n      <th>var_11</th>\n      <th>var_12</th>\n      <th>var_13</th>\n      <th>var_14</th>\n      <th>var_15</th>\n      <th>var_16</th>\n      <th>var_17</th>\n      <th>var_18</th>\n      <th>var_19</th>\n      <th>var_20</th>\n      <th>var_21</th>\n      <th>var_22</th>\n      <th>var_23</th>\n      <th>var_24</th>\n      <th>var_25</th>\n      <th>var_26</th>\n      <th>var_27</th>\n      <th>var_28</th>\n      <th>var_29</th>\n      <th>var_30</th>\n      <th>var_31</th>\n      <th>var_32</th>\n      <th>var_33</th>\n      <th>var_34</th>\n      <th>var_35</th>\n      <th>var_36</th>\n      <th>var_37</th>\n      <th>var_38</th>\n      <th>var_39</th>\n      <th>...</th>\n      <th>var_160</th>\n      <th>var_161</th>\n      <th>var_162</th>\n      <th>var_163</th>\n      <th>var_164</th>\n      <th>var_165</th>\n      <th>var_166</th>\n      <th>var_167</th>\n      <th>var_168</th>\n      <th>var_169</th>\n      <th>var_170</th>\n      <th>var_171</th>\n      <th>var_172</th>\n      <th>var_173</th>\n      <th>var_174</th>\n      <th>var_175</th>\n      <th>var_176</th>\n      <th>var_177</th>\n      <th>var_178</th>\n      <th>var_179</th>\n      <th>var_180</th>\n      <th>var_181</th>\n      <th>var_182</th>\n      <th>var_183</th>\n      <th>var_184</th>\n      <th>var_185</th>\n      <th>var_186</th>\n      <th>var_187</th>\n      <th>var_188</th>\n      <th>var_189</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>105800</th>\n      <td>7.1710</td>\n      <td>0.0780</td>\n      <td>9.7313</td>\n      <td>5.2754</td>\n      <td>8.4569</td>\n      <td>1.0961</td>\n      <td>5.6193</td>\n      <td>19.7863</td>\n      <td>-3.0315</td>\n      <td>6.8223</td>\n      <td>2.5263</td>\n      <td>4.9981</td>\n      <td>13.9677</td>\n      <td>6.8061</td>\n      <td>4.7322</td>\n      <td>14.1186</td>\n      <td>9.0974</td>\n      <td>-11.2240</td>\n      <td>17.9139</td>\n      <td>11.0509</td>\n      <td>14.7752</td>\n      <td>32.8374</td>\n      <td>10.2966</td>\n      <td>3.4888</td>\n      <td>6.2547</td>\n      <td>13.5648</td>\n      <td>-5.7459</td>\n      <td>-0.6498</td>\n      <td>4.3383</td>\n      <td>4.7462</td>\n      <td>-13.1613</td>\n      <td>10.5528</td>\n      <td>-0.7888</td>\n      <td>11.6772</td>\n      <td>11.2966</td>\n      <td>7.9723</td>\n      <td>5.8383</td>\n      <td>8.4460</td>\n      <td>13.3923</td>\n      <td>-4.4930</td>\n      <td>...</td>\n      <td>30.4980</td>\n      <td>5.7451</td>\n      <td>4.6325</td>\n      <td>5.0144</td>\n      <td>1.4196</td>\n      <td>23.7389</td>\n      <td>2.6750</td>\n      <td>-1.3684</td>\n      <td>7.6428</td>\n      <td>5.7646</td>\n      <td>-3.8745</td>\n      <td>-10.4765</td>\n      <td>25.1012</td>\n      <td>-9.9958</td>\n      <td>27.2812</td>\n      <td>6.2933</td>\n      <td>-15.8091</td>\n      <td>10.4539</td>\n      <td>-9.3122</td>\n      <td>2.8948</td>\n      <td>-4.8210</td>\n      <td>8.4080</td>\n      <td>5.0552</td>\n      <td>16.9429</td>\n      <td>14.0333</td>\n      <td>2.5933</td>\n      <td>12.6163</td>\n      <td>-7.2145</td>\n      <td>12.9924</td>\n      <td>1.2416</td>\n      <td>1.3701</td>\n      <td>9.9152</td>\n      <td>2.0668</td>\n      <td>5.3564</td>\n      <td>21.6815</td>\n      <td>-0.1577</td>\n      <td>-1.3003</td>\n      <td>8.4862</td>\n      <td>16.8159</td>\n      <td>-7.3853</td>\n    </tr>\n    <tr>\n      <th>27538</th>\n      <td>12.9988</td>\n      <td>-0.8167</td>\n      <td>9.0578</td>\n      <td>6.1068</td>\n      <td>9.0826</td>\n      <td>-20.3450</td>\n      <td>7.3668</td>\n      <td>20.5931</td>\n      <td>4.4952</td>\n      <td>8.3794</td>\n      <td>-2.6283</td>\n      <td>-11.5343</td>\n      <td>14.2105</td>\n      <td>6.3588</td>\n      <td>10.6629</td>\n      <td>14.2864</td>\n      <td>6.1238</td>\n      <td>3.2445</td>\n      <td>6.4376</td>\n      <td>6.2494</td>\n      <td>20.0997</td>\n      <td>20.3825</td>\n      <td>3.5640</td>\n      <td>3.8544</td>\n      <td>11.0345</td>\n      <td>13.3940</td>\n      <td>6.4604</td>\n      <td>-1.3166</td>\n      <td>5.8386</td>\n      <td>5.3434</td>\n      <td>-15.8937</td>\n      <td>9.8919</td>\n      <td>-3.7623</td>\n      <td>10.7072</td>\n      <td>12.2440</td>\n      <td>13.1534</td>\n      <td>7.4214</td>\n      <td>2.3099</td>\n      <td>11.0870</td>\n      <td>4.6868</td>\n      <td>...</td>\n      <td>46.0527</td>\n      <td>5.8512</td>\n      <td>4.7978</td>\n      <td>6.5647</td>\n      <td>-7.1101</td>\n      <td>29.4211</td>\n      <td>2.4211</td>\n      <td>-10.6893</td>\n      <td>2.5800</td>\n      <td>5.5625</td>\n      <td>-6.7060</td>\n      <td>1.6756</td>\n      <td>13.2715</td>\n      <td>3.4671</td>\n      <td>31.7427</td>\n      <td>15.9629</td>\n      <td>4.2036</td>\n      <td>9.1558</td>\n      <td>17.3793</td>\n      <td>1.3517</td>\n      <td>-4.4944</td>\n      <td>7.8056</td>\n      <td>7.4971</td>\n      <td>9.1212</td>\n      <td>22.2043</td>\n      <td>4.3224</td>\n      <td>2.9281</td>\n      <td>-10.8262</td>\n      <td>18.4998</td>\n      <td>0.1512</td>\n      <td>2.2635</td>\n      <td>9.9614</td>\n      <td>0.9975</td>\n      <td>4.2109</td>\n      <td>19.2522</td>\n      <td>-0.2888</td>\n      <td>2.2767</td>\n      <td>8.4979</td>\n      <td>13.6564</td>\n      <td>-1.1474</td>\n    </tr>\n    <tr>\n      <th>108979</th>\n      <td>11.2861</td>\n      <td>-0.2255</td>\n      <td>17.2522</td>\n      <td>5.8849</td>\n      <td>13.0078</td>\n      <td>4.0183</td>\n      <td>5.0971</td>\n      <td>11.4301</td>\n      <td>-1.7817</td>\n      <td>6.3523</td>\n      <td>-0.7780</td>\n      <td>-10.6161</td>\n      <td>13.5544</td>\n      <td>13.9533</td>\n      <td>7.2666</td>\n      <td>15.0989</td>\n      <td>5.8719</td>\n      <td>-14.9021</td>\n      <td>22.0891</td>\n      <td>15.5638</td>\n      <td>4.7581</td>\n      <td>21.1650</td>\n      <td>9.3463</td>\n      <td>2.1115</td>\n      <td>13.6982</td>\n      <td>13.8076</td>\n      <td>-4.5608</td>\n      <td>-3.5556</td>\n      <td>4.6642</td>\n      <td>6.1699</td>\n      <td>4.9259</td>\n      <td>11.3081</td>\n      <td>-3.9430</td>\n      <td>8.0641</td>\n      <td>12.2030</td>\n      <td>3.5548</td>\n      <td>2.7875</td>\n      <td>5.8096</td>\n      <td>16.6633</td>\n      <td>4.7082</td>\n      <td>...</td>\n      <td>33.3342</td>\n      <td>6.0394</td>\n      <td>2.9413</td>\n      <td>12.9801</td>\n      <td>1.5205</td>\n      <td>20.0853</td>\n      <td>2.8947</td>\n      <td>-19.3527</td>\n      <td>5.2126</td>\n      <td>5.4197</td>\n      <td>-3.7771</td>\n      <td>-3.8371</td>\n      <td>24.2803</td>\n      <td>-9.2187</td>\n      <td>19.6684</td>\n      <td>10.6127</td>\n      <td>-0.9008</td>\n      <td>11.7708</td>\n      <td>3.7611</td>\n      <td>1.9704</td>\n      <td>-8.2891</td>\n      <td>10.3222</td>\n      <td>6.8423</td>\n      <td>8.2890</td>\n      <td>16.0035</td>\n      <td>-3.2908</td>\n      <td>13.5046</td>\n      <td>-14.7954</td>\n      <td>13.5966</td>\n      <td>2.4556</td>\n      <td>7.1895</td>\n      <td>11.7965</td>\n      <td>2.2497</td>\n      <td>10.9765</td>\n      <td>18.8830</td>\n      <td>1.0117</td>\n      <td>1.8379</td>\n      <td>7.3530</td>\n      <td>13.5594</td>\n      <td>6.4416</td>\n    </tr>\n    <tr>\n      <th>184818</th>\n      <td>13.4434</td>\n      <td>-5.7222</td>\n      <td>13.4512</td>\n      <td>4.7605</td>\n      <td>14.5425</td>\n      <td>2.6660</td>\n      <td>5.8011</td>\n      <td>19.3957</td>\n      <td>1.4480</td>\n      <td>7.6016</td>\n      <td>-4.4300</td>\n      <td>-15.8139</td>\n      <td>14.1223</td>\n      <td>16.3147</td>\n      <td>11.0964</td>\n      <td>14.6087</td>\n      <td>5.8165</td>\n      <td>-10.2562</td>\n      <td>13.8512</td>\n      <td>13.7855</td>\n      <td>10.0983</td>\n      <td>36.3466</td>\n      <td>1.5583</td>\n      <td>1.9625</td>\n      <td>7.2798</td>\n      <td>13.4934</td>\n      <td>-5.1827</td>\n      <td>1.0547</td>\n      <td>5.8489</td>\n      <td>1.3562</td>\n      <td>5.1330</td>\n      <td>10.9412</td>\n      <td>2.1192</td>\n      <td>16.5629</td>\n      <td>11.5814</td>\n      <td>-0.9995</td>\n      <td>7.5970</td>\n      <td>7.3915</td>\n      <td>11.5300</td>\n      <td>-4.0754</td>\n      <td>...</td>\n      <td>21.7864</td>\n      <td>5.7288</td>\n      <td>4.0159</td>\n      <td>5.6799</td>\n      <td>-2.0304</td>\n      <td>20.9787</td>\n      <td>2.8468</td>\n      <td>4.4427</td>\n      <td>5.2771</td>\n      <td>5.2363</td>\n      <td>1.5659</td>\n      <td>2.1499</td>\n      <td>19.3227</td>\n      <td>5.1222</td>\n      <td>21.8927</td>\n      <td>13.8721</td>\n      <td>-21.4911</td>\n      <td>13.3503</td>\n      <td>-4.5672</td>\n      <td>4.9113</td>\n      <td>-0.3818</td>\n      <td>11.2787</td>\n      <td>-15.2789</td>\n      <td>4.9551</td>\n      <td>1.3241</td>\n      <td>-5.3471</td>\n      <td>8.9914</td>\n      <td>-7.6601</td>\n      <td>15.2406</td>\n      <td>0.1046</td>\n      <td>-2.5342</td>\n      <td>6.7572</td>\n      <td>4.1626</td>\n      <td>1.5798</td>\n      <td>18.3637</td>\n      <td>1.7772</td>\n      <td>9.0057</td>\n      <td>8.5145</td>\n      <td>18.6113</td>\n      <td>3.9470</td>\n    </tr>\n    <tr>\n      <th>121763</th>\n      <td>7.3488</td>\n      <td>3.2396</td>\n      <td>12.8417</td>\n      <td>8.3421</td>\n      <td>12.0869</td>\n      <td>-0.3122</td>\n      <td>4.1239</td>\n      <td>11.7267</td>\n      <td>0.8601</td>\n      <td>9.4517</td>\n      <td>7.5351</td>\n      <td>-13.3338</td>\n      <td>14.0258</td>\n      <td>5.2542</td>\n      <td>8.0364</td>\n      <td>14.3044</td>\n      <td>5.5817</td>\n      <td>-8.9936</td>\n      <td>3.5019</td>\n      <td>-0.2994</td>\n      <td>24.0453</td>\n      <td>17.4334</td>\n      <td>6.9780</td>\n      <td>3.9353</td>\n      <td>7.8424</td>\n      <td>13.8977</td>\n      <td>10.1185</td>\n      <td>0.5483</td>\n      <td>4.6773</td>\n      <td>8.3812</td>\n      <td>-8.9568</td>\n      <td>13.4227</td>\n      <td>3.3882</td>\n      <td>10.4680</td>\n      <td>11.5528</td>\n      <td>10.8748</td>\n      <td>-0.3079</td>\n      <td>6.2513</td>\n      <td>13.9002</td>\n      <td>0.5789</td>\n      <td>...</td>\n      <td>49.2054</td>\n      <td>5.2230</td>\n      <td>5.7743</td>\n      <td>15.5129</td>\n      <td>1.0193</td>\n      <td>18.2324</td>\n      <td>2.7537</td>\n      <td>-7.4034</td>\n      <td>5.0998</td>\n      <td>5.4153</td>\n      <td>3.5544</td>\n      <td>2.3753</td>\n      <td>8.7023</td>\n      <td>1.4464</td>\n      <td>16.7936</td>\n      <td>6.2366</td>\n      <td>-7.3021</td>\n      <td>9.3020</td>\n      <td>-3.4587</td>\n      <td>3.6592</td>\n      <td>1.6869</td>\n      <td>6.3266</td>\n      <td>-8.0208</td>\n      <td>6.0773</td>\n      <td>1.4794</td>\n      <td>-3.7113</td>\n      <td>9.1823</td>\n      <td>-23.1275</td>\n      <td>17.4792</td>\n      <td>1.6969</td>\n      <td>6.8424</td>\n      <td>3.1530</td>\n      <td>2.0189</td>\n      <td>3.3557</td>\n      <td>22.6439</td>\n      <td>0.2648</td>\n      <td>0.3030</td>\n      <td>8.0920</td>\n      <td>16.3242</td>\n      <td>-10.1387</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Use StratifiedKFold to ensure test and train datasets contains equal percentage of both classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=10, shuffle=False, random_state=2319)\nparam = {\n    'bagging_freq': 5, \n    'bagging_fraction': 0.33,\n    'boost_from_average':'false',   \n    'boost': 'gbdt',\n    'feature_fraction': 0.0405,\n    'learning_rate': 0.083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,     \n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 4,            \n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1\n}\noof = np.zeros(len(train_data))\npredictions = np.zeros(len(test_data))\nfeatures = [c for c in train_data.columns if c not in ['ID_code', 'target']]\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data.values, labels.values)):\n    trn_data = lgb.Dataset(train_data.iloc[trn_idx][features], label=labels.iloc[trn_idx])\n    val_data = lgb.Dataset(train_data.iloc[val_idx][features], label=labels.iloc[val_idx])\n    clf = lgb.train(param, trn_data, 1000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 4000)\n    oof[val_idx] = clf.predict(train_data.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(test_data[features], num_iteration=clf.best_iteration) / folds.n_splits\n","execution_count":6,"outputs":[{"output_type":"stream","text":"Training until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.936435\tvalid_1's auc: 0.894534\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.936193\tvalid_1's auc: 0.893918\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.93693\tvalid_1's auc: 0.887632\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.936294\tvalid_1's auc: 0.892666\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.936533\tvalid_1's auc: 0.896788\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.936112\tvalid_1's auc: 0.899933\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.936449\tvalid_1's auc: 0.898047\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.935903\tvalid_1's auc: 0.897966\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.935815\tvalid_1's auc: 0.898876\nTraining until validation scores don't improve for 4000 rounds.\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's auc: 0.936448\tvalid_1's auc: 0.895264\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_data.iloc[val_idx]['target']\nprint(\"\\n >> CV score: {:<8.5f}\".format(roc_auc_score(target, oof[val_idx])))\n","execution_count":7,"outputs":[{"output_type":"stream","text":"\n >> CV score: 0.89526 \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID_code = test_data[\"ID_code\"]\nsubmission = pd.DataFrame({'ID_code' : ID_code,\n                            'target' : predictions})\nsubmission.to_csv('./version1.csv', index=False)\nsub = pd.read_csv('./version1.csv')\nsub.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"  ID_code    target\n0  test_0  0.117333\n1  test_1  0.212773\n2  test_2  0.211517\n3  test_3  0.226387\n4  test_4  0.030008","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_0</td>\n      <td>0.117333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_1</td>\n      <td>0.212773</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_2</td>\n      <td>0.211517</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_3</td>\n      <td>0.226387</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_4</td>\n      <td>0.030008</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}